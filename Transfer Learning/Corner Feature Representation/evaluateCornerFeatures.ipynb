{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Input\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.core import Activation, Dense, Dropout, Lambda\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def load_vectors(vector_file, prefix_filter=''):\n",
    "    vec_dict = {}\n",
    "    fvec = open(vector_file, \"r\")\n",
    "    for line in tqdm(fvec):\n",
    "            image_name, image_vec = line.strip().split(\"\\t\")\n",
    "            #if prefix_filter != '' and image_name.startswith(prefix_filter):\n",
    "            vec = np.array([float(v) for v in image_vec.split(\",\")])\n",
    "            vec_dict[image_name] = vec\n",
    "    fvec.close()\n",
    "    return vec_dict\n",
    "\n",
    "def batch_to_vectors(batch, vec_size, vec_dict):\n",
    "    X1 = np.zeros((len(batch), vec_size))\n",
    "    X2 = np.zeros((len(batch), vec_size))\n",
    "    Y = np.zeros((len(batch), 2))\n",
    "    for tid in range(len(batch)):\n",
    "        filename, ext = batch[tid][0].strip().split(\".\")\n",
    "        painting1 = vec_dict[batch[tid][0]]\n",
    "        paiting1_a = vec_corner_dict[filename + '_a.' + ext]\n",
    "        paiting1_b = vec_corner_dict[filename + '_b.' + ext]\n",
    "        paiting1_c = vec_corner_dict[filename + '_c.' + ext]\n",
    "        paiting1_d = vec_corner_dict[filename + '_d.' + ext]\n",
    "\n",
    "        filename, ext = batch[tid][0].strip().split(\".\")\n",
    "        painting2 = vec_dict[batch[tid][0]]\n",
    "        paiting2_a = vec_corner_dict[filename + '_a.' + ext]\n",
    "        paiting2_b = vec_corner_dict[filename + '_b.' + ext]\n",
    "        paiting2_c = vec_corner_dict[filename + '_c.' + ext]\n",
    "        paiting2_d = vec_corner_dict[filename + '_d.' + ext]\n",
    "        \n",
    "        X1[tid] = np.concatenate([painting1, paiting1_a, paiting1_b, paiting1_c, paiting1_d], axis=0)\n",
    "        X2[tid] = np.concatenate([painting2, paiting2_a, paiting2_b, paiting2_c, paiting2_d], axis=0)\n",
    "        \n",
    "        Y[tid] = [1, 0] if batch[tid][2] == 0 else [0, 1]\n",
    "    return ([X1, X2], Y)\n",
    "\n",
    "def data_generator(triples, vec_size, vec_dict, batch_size=32):\n",
    "    while True:\n",
    "        # shuffle once per batch\n",
    "        indices = np.random.permutation(np.arange(len(triples)))\n",
    "        num_batches = len(triples) // batch_size\n",
    "        for bid in range(num_batches):\n",
    "            batch_indices = indices[bid * batch_size: (bid + 1) * batch_size]\n",
    "            batch = [triples[i] for i in batch_indices]\n",
    "            yield batch_to_vectors(batch, vec_size, vec_dict)\n",
    "            \n",
    "def evaluate_model(model, test_gen, triple_len):\n",
    "    ytrue, ypred = [], []\n",
    "    num_test_steps = triple_len // BATCH_SIZE\n",
    "    for i in range(num_test_steps):\n",
    "        (X1, X2), Y = next(test_gen)\n",
    "        Y_ = model.predict([X1, X2])\n",
    "        ytrue.extend(np.argmax(Y, axis=1).tolist())\n",
    "        ypred.extend(np.argmax(Y_, axis=1).tolist())\n",
    "    accuracy = accuracy_score(ytrue, ypred)\n",
    "    print(\"\\nAccuracy: {:.3f}\".format(accuracy))\n",
    "    print(\"\\nConfusion Matrix\")\n",
    "    print(confusion_matrix(ytrue, ypred))\n",
    "    print(\"\\nClassification Report\")\n",
    "    print(classification_report(ytrue, ypred))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = '/home/nelssalminen/painters/data/'\n",
    "OUTPUT_DIR = '/home/nelssalminen/painters/data/output/'\n",
    "MODEL_DIR = '/home/nelssalminen/painters/data/scratch/models'\n",
    "CORNER_IMG_DIR = '/home/nelssalminen/painters/fullres_data/output/'\n",
    "\n",
    "VECTOR_SIZE = 10240\n",
    "VECTOR_FILE = os.path.join(OUTPUT_DIR, 'xception-vectors_alldata.tsv')\n",
    "VECTOR_FILE_CORNERS = os.path.join(CORNER_IMG_DIR, \"xception-segment-vectors-gpu.tsv\")\n",
    "\n",
    "vec_corner_dict = load_vectors(VECTOR_FILE_CORNERS)\n",
    "vec_dict = load_vectors(VECTOR_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILE = os.path.join(MODEL_DIR, 'xception_corner-cat-best.h5')\n",
    "model = load_model(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('/home/nelssalminen/painters/data/all_data_info_custom.csv')\n",
    "ALL_IMG_LIST = all_data['new_filename'].tolist()\n",
    "\n",
    "CORRUPTED_IMG_LIST_PATH = os.path.join(CORNER_IMG_DIR, \"error_images.tsv\")\n",
    "CORRUPTED_IMG_LIST = []\n",
    "with open(CORRUPTED_IMG_LIST_PATH) as f:\n",
    "    for line in f:\n",
    "        line = line.replace('\\n','')\n",
    "        CORRUPTED_IMG_LIST.append(line)\n",
    "        \n",
    "list_error_anyway = []\n",
    "for imgpath in ALL_IMG_LIST:\n",
    "    folder_file, ext = imgpath.strip().split(\".\")\n",
    "    full_exists = (imgpath in vec_dict)\n",
    "    a_exists = (str(folder_file + '_a.' + ext) in vec_corner_dict)\n",
    "    b_exists = (str(folder_file + '_b.' + ext) in vec_corner_dict)\n",
    "    c_exists = (str(folder_file + '_c.' + ext) in vec_corner_dict)\n",
    "    d_exists = (str(folder_file + '_d.' + ext) in vec_corner_dict)\n",
    "    if not(full_exists and a_exists and b_exists and c_exists and d_exists):\n",
    "        if imgpath not in CORRUPTED_IMG_LIST:\n",
    "            counter = counter + 1\n",
    "            list_error_anyway.append(imgpath)\n",
    "\n",
    "CORRUPTED_IMG_LIST = CORRUPTED_IMG_LIST + list_error_anyway\n",
    "\n",
    "def get_triples(image_dir, dat, filename_label='filename', path_prefix=''):\n",
    "    image_groups = {}\n",
    "    for index, row in dat.iterrows():\n",
    "        img_name = row[filename_label]\n",
    "        group_name = row['artist']\n",
    "        if (path_prefix + img_name) not in CORRUPTED_IMG_LIST:\n",
    "            if group_name in image_groups:\n",
    "                image_groups[group_name].append(path_prefix + img_name)\n",
    "            else:\n",
    "                image_groups[group_name] = [path_prefix + img_name]\n",
    "    num_sims = 0\n",
    "    image_triples = []\n",
    "    group_list = sorted(list(image_groups.keys()))\n",
    "    for i, g in enumerate(group_list):\n",
    "            if num_sims % 100 == 0:\n",
    "                    print(\"Generated {:d} pos + {:d} neg = {:d} total image triples\"\n",
    "                                .format(num_sims, num_sims, 2*num_sims))\n",
    "            images_in_group = image_groups[g]\n",
    "            sim_pairs_it = itertools.combinations(images_in_group, 2)\n",
    "            # For each similar pair, generate a corresponding different pair\n",
    "            for ref_image, sim_image in sim_pairs_it:\n",
    "                image_triples.append((ref_image, sim_image, 1))\n",
    "                num_sims += 1\n",
    "                while True:\n",
    "                        j = np.random.randint(low=0, high=len(group_list), size=1)[0]\n",
    "                        if j != i:\n",
    "                                break\n",
    "                dif_image_candidates = image_groups[group_list[j]]\n",
    "                k = np.random.randint(low=0, high=len(dif_image_candidates), size=1)[0]\n",
    "                dif_image = dif_image_candidates[k]\n",
    "                image_triples.append((ref_image, dif_image, 0))\n",
    "\n",
    "    print(\"Generated {:d} pos + {:d} neg = {:d} total image triples\"\n",
    "                .format(num_sims, num_sims, 2*num_sims))\n",
    "    return image_triples\n",
    "\n",
    "test_data_seen = pd.read_csv('/home/nelssalminen/painters/data/test_info-train_only.csv')\n",
    "test_data_unseen = pd.read_csv('/home/nelssalminen/painters/data/test_info-test_only.csv')\n",
    "test_data_mix = pd.read_csv('/home/nelssalminen/painters/data/test_info.csv')\n",
    "\n",
    "test_triples_seen = get_triples(IMG_DIR, test_data_seen, 'new_filename');\n",
    "test_triples_unseen = get_triples(IMG_DIR, test_data_unseen, 'new_filename');\n",
    "test_triples_mix = get_triples(IMG_DIR, test_data_mix, 'new_filename');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "test_gen_seen = data_generator(test_triples_seen, VECTOR_SIZE, vec_dict, BATCH_SIZE)\n",
    "final_accuracy = evaluate_model(model, test_gen_seen, len(test_triples_seen))\n",
    "\n",
    "test_gen_unseen = data_generator(test_triples_unseen, VECTOR_SIZE, vec_dict, BATCH_SIZE)\n",
    "final_accuracy = evaluate_model(model, test_gen_unseen, len(test_triples_unseen))\n",
    "\n",
    "test_gen_mix = data_generator(test_triples_mix, VECTOR_SIZE, vec_dict, BATCH_SIZE)\n",
    "final_accuracy = evaluate_model(model, test_gen_mix, len(test_triples_mix))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
